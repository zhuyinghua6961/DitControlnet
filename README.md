# ControlNet-DiT: æ¡ä»¶å›¾åƒç”Ÿæˆ

åŸºäº DiT (Diffusion Transformer) å’Œ ControlNet çš„æ¡ä»¶å›¾åƒç”Ÿæˆæ¨¡å‹ï¼Œæ”¯æŒ fill50k æ•°æ®é›†çš„é«˜æ•ˆè®­ç»ƒã€‚

## ç½‘ç»œæ¶æ„è¯¦è§£

### æ•´ä½“æ¶æ„æ¦‚è¿°

ControlNet-DiT é‡‡ç”¨"ä¸»å¹²ç½‘ç»œ + æ§åˆ¶åˆ†æ”¯"çš„åŒè·¯å¾„æ¶æ„ï¼Œç»“åˆäº† Transformer çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›å’Œ ControlNet çš„ç²¾ç¡®æ§åˆ¶èƒ½åŠ›ã€‚

```
è¾“å…¥å›¾åƒ (å™ªå£°)          æ¡ä»¶å›¾åƒ (Canny/Depthç­‰)
      â†“                           â†“
 Patch Embed              Condition Encoder
      â†“                           â†“
 Position Embed          Feature Extraction
      â†“                           â†“
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  Control Blocks  â”€â”€â”€â†’ Zero Convs
      â†“                     â†“                    â†“
 DiT Blocks  â†â”€â”€â”€â”€â”€â”€â”€â”€â”€ Residual Injection â”€â”€â”€â”€â”€â”˜
      â†“
 Final Norm
      â†“
 Output Proj
      â†“
  é¢„æµ‹å™ªå£°/å›¾åƒ
```

### æ ¸å¿ƒæ¨¡å—è¯¦è§£

#### 1. **ConditionEncoder (æ¡ä»¶ç¼–ç å™¨)**
**ä½œç”¨**: å°†æ¡ä»¶å›¾åƒ(å¦‚ Canny è¾¹ç¼˜ã€æ·±åº¦å›¾)ç¼–ç ä¸ºä¸ Transformer å¯¹é½çš„ç‰¹å¾è¡¨ç¤º

**ç»“æ„**:
```python
è¾“å…¥: (B, 3, 512, 512) RGB æ¡ä»¶å›¾åƒ
  â†“
Conv2d(3â†’64, stride=2)    # 512 â†’ 256
  â†“ SiLU
Conv2d(64â†’128, stride=2)  # 256 â†’ 128
  â†“ SiLU
Conv2d(128â†’256, stride=2) # 128 â†’ 64
  â†“ SiLU
Conv2d(256â†’1152, stride=1) # ç‰¹å¾æ˜ å°„åˆ° hidden_size
  â†“
Patch Embed(2x2, stride=2) # 64 â†’ 32 (patchify)
  â†“
è¾“å‡º: (B, 1024, 1152) # 1024 ä¸ª tokenï¼Œæ¯ä¸ªç»´åº¦ 1152
```

**å…³é”®ç‰¹æ€§**:
- 3 æ¬¡ä¸‹é‡‡æ ·å·ç§¯å°†ç©ºé—´åˆ†è¾¨ç‡ä» 512 é™è‡³ 64
- æœ€ç»ˆ patch embed å°†ç‰¹å¾å›¾è½¬ä¸º token åºåˆ—
- è¾“å‡ºç»´åº¦ä¸ä¸» Transformer çš„ hidden_size å¯¹é½

#### 2. **DiTBlock (DiT Transformer å—)**
**ä½œç”¨**: åŸºäº Adaptive Layer Normalization (AdaLN) çš„ Transformer å—ï¼Œå®ç°æ¡ä»¶æ³¨å…¥

**ç»“æ„**:
```python
è¾“å…¥: x (B, N, D), c (B, D) æ¡ä»¶åµŒå…¥
  â†“
AdaLN Modulation â†’ (shift_msa, scale_msa, gate_msa, 
                     shift_mlp, scale_mlp, gate_mlp)
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Self-Attention åˆ†æ”¯:             â”‚
â”‚   LayerNorm(x)                  â”‚
â”‚   â†’ AdaLN(shift, scale)         â”‚
â”‚   â†’ MultiheadAttention          â”‚
â”‚   â†’ Gate(gate_msa)              â”‚
â”‚   â†’ Residual Add                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MLP åˆ†æ”¯:                        â”‚
â”‚   LayerNorm(x)                  â”‚
â”‚   â†’ AdaLN(shift, scale)         â”‚
â”‚   â†’ Linearâ†’GELUâ†’Linear          â”‚
â”‚   â†’ Gate(gate_mlp)              â”‚
â”‚   â†’ Residual Add                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
è¾“å‡º: (B, N, D)
```

**å…³é”®ç‰¹æ€§**:
- **AdaLN (Adaptive Layer Normalization)**: é€šè¿‡ä»¿å°„å˜æ¢ `scale` å’Œ `shift` æ³¨å…¥æ—¶é—´æ­¥å’Œæ¡ä»¶ä¿¡æ¯
- **é—¨æ§æœºåˆ¶ (Gating)**: é€šè¿‡ `gate` å‚æ•°æ§åˆ¶æ¯ä¸ªåˆ†æ”¯çš„è´¡çŒ®
- **MLP æ‰©å±•æ¯”ä¾‹**: é»˜è®¤ 4.0ï¼Œå³éšè—å±‚ç»´åº¦æ˜¯è¾“å…¥çš„ 4 å€

#### 3. **ControlNet åˆ†æ”¯**
**ä½œç”¨**: å…‹éš†ä¸» Transformer çš„å‰ N ä¸ªå—ï¼Œæ„å»ºç‹¬ç«‹çš„æ§åˆ¶è·¯å¾„

**ç»“æ„**:
```python
n_control_blocks = 14  # é€šå¸¸ä½¿ç”¨å‰ 14 å±‚

æ§åˆ¶åˆ†æ”¯:
  æ¡ä»¶ç‰¹å¾ + ä¸»å¹²éšè—çŠ¶æ€
    â†“
  Cloned Block 1 â†’ Zero Linear 1 â†’ Residual 1
    â†“
  Cloned Block 2 â†’ Zero Linear 2 â†’ Residual 2
    â†“
    ...
    â†“
  Cloned Block 14 â†’ Zero Linear 14 â†’ Residual 14
    â†“
  æ³¨å…¥åˆ°ä¸»å¹²å¯¹åº”å±‚
```

**å…³é”®ç‰¹æ€§**:
- **é›¶åˆå§‹åŒ–çº¿æ€§å±‚ (ZeroLinear)**: 
  - è®­ç»ƒåˆæœŸä¸å½±å“ä¸»ç½‘ç»œï¼Œä¿è¯ç¨³å®šæ€§
  - æƒé‡å’Œåç½®åˆå§‹åŒ–ä¸º 0
  - éšè®­ç»ƒé€æ­¥å­¦ä¹ æ§åˆ¶ä¿¡å·
  
- **é€å—æ®‹å·®æ³¨å…¥**: 
  - æ¯ä¸ªæ§åˆ¶å—çš„è¾“å‡ºé€šè¿‡ Zero Linear åæ³¨å…¥ä¸»å¹²
  - ä¿æŒç»†ç²’åº¦çš„ç©ºé—´æ§åˆ¶èƒ½åŠ›

#### 4. **å®Œæ•´ ControlNetDiT æ¨¡å‹**

**ä¸»è¦ç»„ä»¶**:

| ç»„ä»¶ | è¾“å…¥ | è¾“å‡º | ä½œç”¨ |
|------|------|------|------|
| `patch_embed` | (B, C, H, W) | (B, N, D) | å°†å›¾åƒåˆ†å‰²ä¸º patch åºåˆ— |
| `pos_embed` | - | (1, N, D) | å¯å­¦ä¹ çš„ä½ç½®ç¼–ç  |
| `control_embed` | (B, C, H, W) | (B, N, D) | æ¡ä»¶å›¾åƒçš„ patch åµŒå…¥ |
| `time_embed` | (B,) | (B, D) | æ—¶é—´æ­¥çš„æ­£å¼¦ä½ç½®ç¼–ç  |
| `blocks` | (B, N, D) | (B, N, D) | ä¸» Transformer å—åºåˆ— |
| `norm` | (B, N, D) | (B, N, D) | è¾“å‡ºå‰çš„ LayerNorm |
| `final_proj` | (B, N, D) | (B, N, CÃ—PÂ²) | æŠ•å½±å›å›¾åƒç©ºé—´ |

**å‰å‘ä¼ æ’­æµç¨‹**:
```python
1. è¾“å…¥å¤„ç†:
   x_patches = patch_embed(å™ªå£°å›¾åƒ) + pos_embed
   cond_patches = control_embed(æ¡ä»¶å›¾åƒ)
   
2. æ—¶é—´å’Œæ¡ä»¶åµŒå…¥:
   t_embed = time_embed(timestep)
   c_embed = t_embed + mean(cond_patches)  # å…¨å±€æ¡ä»¶
   
3. Transformer å¤„ç†:
   for block in blocks:
       x_patches = block(x_patches, c_embed)
   
4. è¾“å‡ºæŠ•å½±:
   x_patches = norm(x_patches)
   x_patches = final_proj(x_patches)
   x_out = rearrange(x_patches) â†’ (B, C, H, W)
```

### æ‰©æ•£è¿‡ç¨‹è¯¦è§£

#### æ—¶é—´æ­¥ç¼–ç  (Timestep Embedding)
```python
def timestep_embedding(t, dim):
    """æ­£å¼¦ä½ç½®ç¼–ç """
    half_dim = dim // 2
    emb = log(10000) / (half_dim - 1)
    emb = exp(arange(half_dim) * -emb)
    emb = t[:, None] * emb[None, :]
    emb = concat([sin(emb), cos(emb)], dim=-1)
    return emb  # (B, dim)
```

**ä½œç”¨**: å°†ç¦»æ•£æ—¶é—´æ­¥ t âˆˆ [0, 1000] ç¼–ç ä¸ºè¿ç»­å‘é‡è¡¨ç¤º

#### æ‰©æ•£è°ƒåº¦ (Diffusion Schedule)

æ”¯æŒä¸¤ç§è°ƒåº¦ç±»å‹:

**1. Linear Schedule**:
```python
Î²_t = linear_interp(Î²_start, Î²_end, t/T)
Î±_t = 1 - Î²_t
á¾±_t = âˆ(Î±_s) for s=1 to t
```

**2. Cosine Schedule**:
```python
á¾±_t = cosÂ²((t/T + s)/(1+s) Ã— Ï€/2)
Î²_t = 1 - (á¾±_t / á¾±_{t-1})
```

**å…³é”®å‚æ•°**:
- `num_timesteps`: 1000 (æ€»æ‰©æ•£æ­¥æ•°)
- `beta_start`: 0.0001 (å™ªå£°èµ·å§‹å¼ºåº¦)
- `beta_end`: 0.02 (å™ªå£°ç»“æŸå¼ºåº¦)

### æŸå¤±å‡½æ•°

æ”¯æŒå¤šç§æŸå¤±ç±»å‹:

| æŸå¤±ç±»å‹ | å…¬å¼ | é€‚ç”¨åœºæ™¯ |
|---------|------|---------|
| MSE | `L = mean((pred - target)Â²)` | æ ‡å‡†æ‰©æ•£æ¨¡å‹ |
| L1 | `L = mean(|pred - target|)` | å¯¹å¼‚å¸¸å€¼æ›´é²æ£’ |
| Huber | `L = smoothL1(pred, target)` | ç»“åˆ L1 å’Œ L2 ä¼˜åŠ¿ |

### è®­ç»ƒç­–ç•¥

#### æ•°æ®å¢å¼º
```python
# ç©ºé—´å¯¹é½å¢å¼º (æ¡ä»¶å’Œç›®æ ‡åŒæ­¥å˜æ¢)
- éšæœºæ°´å¹³ç¿»è½¬ (p=0.5)
- éšæœºå‚ç›´ç¿»è½¬ (p=0.5)

# å¼‚æ„æ’å€¼
- ç›®æ ‡å›¾åƒ: Bilinear (å¹³æ»‘)
- æ¡ä»¶å›¾åƒ: Nearest (ä¿æŒè¾¹ç¼˜é”åˆ©)
```

#### ä¼˜åŒ–å™¨é…ç½®
```yaml
optimizer: AdamW
learning_rate: 1e-5  # å¾®è°ƒæ¨èæ›´å° LR
weight_decay: 0.01
gradient_clip_norm: 1.0
warmup_steps: 1000
```

#### å†…å­˜ä¼˜åŒ–
- **æ¢¯åº¦æ£€æŸ¥ç‚¹**: å‡å°‘ ~40% æ˜¾å­˜
- **æ··åˆç²¾åº¦ (bf16/fp16)**: åŠ é€Ÿ ~2x
- **æ¢¯åº¦ç´¯ç§¯**: æ¨¡æ‹Ÿå¤§ batch size
- **8-bit AdamW**: èŠ‚çœä¼˜åŒ–å™¨æ˜¾å­˜

## é¡¹ç›®ç»“æ„

```
controlnetDiT/
â”œâ”€â”€ config/                 # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ config.yaml        # ä¸»è¦é…ç½®æ–‡ä»¶
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ dataset/               # æ•°æ®é›†ç›¸å…³
â”‚   â”œâ”€â”€ data/             # JSONLæ•°æ®æ–‡ä»¶
â”‚   â”œâ”€â”€ dataset_fill50k/  # åŸå§‹æ•°æ®é›†
â”‚   â”œâ”€â”€ dataset.py        # æ•°æ®é›†ç±»å’Œæ•°æ®åŠ è½½å™¨
â”‚   â”œâ”€â”€ preprocess_data.py # æ•°æ®é¢„å¤„ç†è„šæœ¬
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/                # æ¨¡å‹æ¶æ„
â”‚   â”œâ”€â”€ controlnet_dit.py # ControlNet-DiTæ¨¡å‹
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ scripts/               # è®­ç»ƒå’Œè¯„ä¼°è„šæœ¬
â”‚   â”œâ”€â”€ train_controlnet_dit.py # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ inference.py      # æ¨ç†è„šæœ¬
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ utils/                 # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ training.py       # è®­ç»ƒå·¥å…·å‡½æ•°
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ checkpoints/           # æ¨¡å‹æ£€æŸ¥ç‚¹
â”œâ”€â”€ logs/                  # æ—¥å¿—æ–‡ä»¶
â”œâ”€â”€ requirements.txt       # ä¾èµ–åŒ…
â””â”€â”€ README.md             # é¡¹ç›®è¯´æ˜
```

## å®éªŒé˜¶æ®µ

### ç¬¬ä¸€é˜¶æ®µï¼šBaseline åŸºå‡†éªŒè¯ âœ…
ä½¿ç”¨ PixArt-alpha-XL-2 (0.6B) åœ¨ Fill50k æ•°æ®é›†ä¸Šè¿›è¡ŒåŸºå‡†è®­ç»ƒï¼ŒéªŒè¯ RTX 3090Ti ç¯å¢ƒå¯è¡Œæ€§ã€‚

**å¿«é€Ÿå¼€å§‹**:
```bash
accelerate launch scripts/train_baseline.py \
  --dataset_name="./dataset/data" \
  --output_dir="./output/baseline" \
  --gradient_checkpointing \
  --use_8bit_adam \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --mixed_precision="bf16"
```

ğŸ“– [è¯¦ç»†æ–‡æ¡£](BASELINE_README.md)

### ç¬¬äºŒé˜¶æ®µï¼šAdaLN Modulation åˆ›æ–° (è§„åˆ’ä¸­)
å°† Element-wise Add æ›¿æ¢ä¸º AdaLN Modulationï¼Œè§‚å¯Ÿæ€§èƒ½æå‡ã€‚

### ç¬¬ä¸‰é˜¶æ®µï¼šå®Œæ•´è¯„ä¼°ä½“ç³» (âœ… å·²å®ç°)
å®ç°å­¦æœ¯çº§è¯„ä¼°æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ç”Ÿæˆè´¨é‡ã€æ§åˆ¶ç²¾åº¦ã€è¯­ä¹‰å¯¹é½å’Œå·¥ç¨‹æ•ˆç‡ã€‚

**æ ¸å¿ƒæŒ‡æ ‡**:
- **æ§åˆ¶ç²¾åº¦**: MSE, SSIM, IoU (è¾¹ç¼˜é‡åˆåº¦)
- **ç”Ÿæˆè´¨é‡**: FID, Inception Score
- **è¯­ä¹‰å¯¹é½**: CLIP Score
- **å·¥ç¨‹æ•ˆç‡**: å‚æ•°é‡ã€æ˜¾å­˜å ç”¨ã€è®­ç»ƒé€Ÿåº¦

**ä½¿ç”¨æ–¹æ³•**:
```bash
python scripts/evaluation.py \
  --generated_images_dir ./output/generated \
  --condition_images_dir ./data/conditions \
  --real_images_dir ./data/real \
  --output_file evaluation_results.json
```

ğŸ“– [å®Œæ•´è¯„ä¼°æŒ‡å—](EVALUATION_GUIDE.md)

### ç¬¬å››é˜¶æ®µï¼šStep-0 æ•°å€¼éªŒè¯ (âœ… å·²å®ç°)
åœ¨æ­£å¼è®­ç»ƒå‰è‡ªåŠ¨éªŒè¯æ¨¡å‹åˆå§‹åŒ–ï¼Œç¡®ä¿é›¶åˆå§‹åŒ–ï¼ˆZero-Linearï¼‰æˆåŠŸä¸”è¾“å‡ºæ­£å¸¸ã€‚

**éªŒè¯å†…å®¹**:
- âœ… æ£€æŸ¥æ¨¡å‹è¾“å‡ºæ˜¯å¦åŒ…å« NaN
- âœ… éªŒè¯è¾“å‡ºæ•°å€¼èŒƒå›´æ˜¯å¦æ­£å¸¸
- âœ… ç¡®ä¿ ZeroLinear åˆå§‹åŒ–ç”Ÿæ•ˆ
- âœ… é€‚ç”¨äº Baseline å’Œ AdaLN ä¸¤ç§æ¨¡å¼

**è‡ªåŠ¨æ‰§è¡Œ**: è®­ç»ƒè„šæœ¬å¯åŠ¨æ—¶è‡ªåŠ¨è¿è¡Œï¼Œæ— éœ€æ‰‹åŠ¨å¹²é¢„ã€‚

## å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè®¾ç½®
```bash
pip install -r requirements.txt
```

### æ•°æ®å‡†å¤‡
```bash
# è¿è¡Œæ•°æ®é¢„å¤„ç†
python dataset/preprocess_data.py

# éªŒè¯æ•°æ®
python -c "from dataset import create_dataloaders; print('æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ')"
```

### è®­ç»ƒæ¨¡å‹
```bash
# å¼€å§‹è®­ç»ƒ
python scripts/train_controlnet_dit.py --config config/config.yaml

# ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
python scripts/train_controlnet_dit.py --config config/config.yaml --resume checkpoints/best_model.pth
```

### æ¨ç†
```bash
# è¿è¡Œæ¨ç†è„šæœ¬
python scripts/inference.py --checkpoint checkpoints/best_model.pth --input_image path/to/condition.png
```

## é…ç½®è¯´æ˜

ä¸»è¦é…ç½®æ–‡ä»¶ä½äº `config/config.yaml`ï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

- **model**: æ¨¡å‹æ¶æ„å‚æ•° (ç»´åº¦ã€å¤´æ•°ã€æ·±åº¦ç­‰)
- **training**: è®­ç»ƒè¶…å‚æ•° (å­¦ä¹ ç‡ã€æ‰¹æ¬¡å¤§å°ã€epochæ•°ç­‰)
- **data**: æ•°æ®è·¯å¾„å’Œé¢„å¤„ç†å‚æ•°
- **diffusion**: æ‰©æ•£è¿‡ç¨‹å‚æ•°
- **hardware**: ç¡¬ä»¶å’Œä¼˜åŒ–è®¾ç½®
- **logging**: æ—¥å¿—å’Œæ£€æŸ¥ç‚¹è®¾ç½®

## æ•°æ®é›†

ä½¿ç”¨ fill50k æ•°æ®é›†ï¼ŒåŒ…å«50,000å¯¹æ¡ä»¶-ç›®æ ‡å›¾åƒå¯¹ï¼š

- **è®­ç»ƒé›†**: 45,000å¯¹
- **éªŒè¯é›†**: 5,000å¯¹
- **åˆ†è¾¨ç‡**: 512x512
- **æ ¼å¼**: PNGå›¾åƒï¼ŒJSONLç´¢å¼•

## ç¡¬ä»¶è¦æ±‚

- **GPU**: RTX 3090 Ti æˆ–æ›´é«˜ (æ¨è24GBæ˜¾å­˜)
- **RAM**: è‡³å°‘32GB
- **å­˜å‚¨**: è‡³å°‘500GB (æ•°æ®é›† + æ£€æŸ¥ç‚¹)

## æ€§èƒ½ä¼˜åŒ–

- **æ‰¹æ¬¡å¤§å°**: 2 (é€‚åˆRTX 3090 Ti)
- **æ··åˆç²¾åº¦**: è‡ªåŠ¨å¯ç”¨
- **æ¢¯åº¦ç´¯ç§¯**: æ”¯æŒå¤§batchè®­ç»ƒ
- **å¤šè¿›ç¨‹åŠ è½½**: 4ä¸ªworkerè¿›ç¨‹

## å®éªŒè·Ÿè¸ª

æ”¯æŒ Weights & Biases å®éªŒè·Ÿè¸ªï¼š

```yaml
logging:
  use_wandb: true
  wandb_project: "controlnet-dit"
```

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†æ­¤ä»£ç ï¼Œè¯·å¼•ç”¨ï¼š

```
ControlNet-DiT: Conditional Image Generation with Diffusion Transformers
```

## è®¸å¯è¯

MIT License

## é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®å®ç°äº† ControlNet-DiT æ¨¡å‹ï¼Œç”¨äºåŸºäºæ¡ä»¶å›¾åƒç”Ÿæˆç›®æ ‡å›¾åƒã€‚æ¨¡å‹ç»“åˆäº†ï¼š
- **DiT (Diffusion Transformer)**: ä½¿ç”¨ Transformer æ¶æ„çš„æ‰©æ•£æ¨¡å‹
- **ControlNet**: é€šè¿‡æ¡ä»¶å›¾åƒæ§åˆ¶ç”Ÿæˆè¿‡ç¨‹
- **fill50k æ•°æ®é›†**: 50,000 å¯¹æ¡ä»¶-ç›®æ ‡å›¾åƒå¯¹

## ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 2.1.0+ (CUDA 12.1 æ¨è)
- RTX 3090 Ti æˆ–ç±»ä¼¼ GPU
- 16GB+ VRAM

## å®‰è£…ä¾èµ–

```bash
# æ¿€æ´» Conda ç¯å¢ƒ
conda activate DitControlnet

# å®‰è£… Python åŒ…
pip install -r requirements.txt

# å¯é€‰ï¼šå®‰è£… flash-attn ä»¥æå‡æ€§èƒ½
pip install flash-attn --no-build-isolation
```

## æ•°æ®å‡†å¤‡

é¡¹ç›®å·²åŒ…å«å¤„ç†å¥½çš„ fill50k æ•°æ®é›†ï¼š

```
fill50k/
â”œâ”€â”€ source/     # æ¡ä»¶å›¾åƒ (50,000 å¼ )
â””â”€â”€ target/     # ç›®æ ‡å›¾åƒ (50,000 å¼ )
```

å¦‚æœéœ€è¦é‡æ–°å¤„ç†æ•°æ®ï¼š

```bash
# ä¸‹è½½åŸå§‹æ•°æ®é›†
python download_fill50k.py

# å¤„ç† Parquet æ ¼å¼æ•°æ®
python process_fill50k.py
```

## è®­ç»ƒæ¨¡å‹

### åŸºæœ¬è®­ç»ƒ

```bash
# ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
python train_controlnet_dit.py
```

### è‡ªå®šä¹‰é…ç½®

ç¼–è¾‘ `config.yaml` æ–‡ä»¶è°ƒæ•´è®­ç»ƒå‚æ•°ï¼Œç„¶åè¿è¡Œï¼š

```bash
python train_controlnet_dit.py --config config.yaml
```

### ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

```bash
python train_controlnet_dit.py --resume checkpoints/checkpoint_epoch_10.pth
```

## æ¨ç†ç”Ÿæˆ

ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹ç”Ÿæˆå›¾åƒï¼š

```bash
python inference.py \
    --checkpoint checkpoints/best_model.pth \
    --condition_image fill50k/source/000000.png \
    --output_path generated.png \
    --num_steps 50
```

## é…ç½®è¯´æ˜

### æ¨¡å‹é…ç½® (config.yaml)

```yaml
model:
  img_size: 256          # å›¾åƒå°ºå¯¸
  patch_size: 16         # Patch å¤§å°
  dim: 768              # æ¨¡å‹ç»´åº¦
  depth: 12             # Transformer å±‚æ•°
  num_heads: 12         # æ³¨æ„åŠ›å¤´æ•°
  mlp_ratio: 4.0        # MLP æ‰©å±•æ¯”ä¾‹

training:
  batch_size: 4         # æ‰¹æ¬¡å¤§å°
  num_epochs: 100       # è®­ç»ƒè½®æ•°
  learning_rate: 1e-4   # å­¦ä¹ ç‡
  weight_decay: 0.01    # æƒé‡è¡°å‡
  gradient_clip_norm: 1.0  # æ¢¯åº¦è£å‰ª

diffusion:
  num_timesteps: 1000   # æ‰©æ•£æ­¥æ•°
  beta_start: 0.0001    # Beta èµ·å§‹å€¼
  beta_end: 0.02        # Beta ç»“æŸå€¼
  schedule_type: "linear"  # è°ƒåº¦ç±»å‹
```

## é¡¹ç›®ç»“æ„

```
controlnetDiT/
â”œâ”€â”€ train_controlnet_dit.py    # è®­ç»ƒè„šæœ¬
â”œâ”€â”€ inference.py              # æ¨ç†è„šæœ¬
â”œâ”€â”€ config.yaml              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ requirements.txt         # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ verify_enviroment.py     # ç¯å¢ƒéªŒè¯
â”œâ”€â”€ download_fill50k.py      # æ•°æ®ä¸‹è½½
â”œâ”€â”€ process_fill50k.py       # æ•°æ®å¤„ç†
â”œâ”€â”€ fill50k/                # å¤„ç†åçš„æ•°æ®é›†
â”‚   â”œâ”€â”€ source/            # æ¡ä»¶å›¾åƒ
â”‚   â””â”€â”€ target/            # ç›®æ ‡å›¾åƒ
â”œâ”€â”€ checkpoints/            # æ¨¡å‹æ£€æŸ¥ç‚¹
â”œâ”€â”€ logs/                  # è®­ç»ƒæ—¥å¿—
â””â”€â”€ samples/               # ç”Ÿæˆæ ·æœ¬
```

## è®­ç»ƒç›‘æ§

é¡¹ç›®æ”¯æŒ Weights & Biases è¿›è¡Œè®­ç»ƒç›‘æ§ï¼š

1. ç¡®ä¿ `config.yaml` ä¸­ `logging.use_wandb: true`
2. ç™»å½• W&B: `wandb login`
3. è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨è®°å½•æŸå¤±ã€å­¦ä¹ ç‡ç­‰æŒ‡æ ‡

## æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–
- ä½¿ç”¨ `xformers` è¿›è¡Œé«˜æ•ˆæ³¨æ„åŠ›è®¡ç®—
- æ”¯æŒæ··åˆç²¾åº¦è®­ç»ƒ (`mixed_precision: true`)
- æ¢¯åº¦ç´¯ç§¯ä»¥å¤„ç†å¤§æ‰¹é‡

### é€Ÿåº¦ä¼˜åŒ–
- ä½¿ç”¨ `torch.compile` åŠ é€Ÿæ¨ç† (PyTorch 2.0+)
- å¤šè¿›ç¨‹æ•°æ®åŠ è½½ (`num_workers: 4`)
- ä¼˜åŒ–çš„æ‰©æ•£è°ƒåº¦

## æ•…éšœæ’é™¤

### CUDA å†…å­˜ä¸è¶³
- å‡å° `batch_size`
- å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
- ä½¿ç”¨ `bitsandbytes` è¿›è¡Œé‡åŒ–

### è®­ç»ƒä¸ç¨³å®š
- å¯ç”¨æ¢¯åº¦è£å‰ª (`gradient_clip_norm: 1.0`)
- è°ƒæ•´å­¦ä¹ ç‡
- æ£€æŸ¥æ•°æ®é¢„å¤„ç†

### æ¨ç†è´¨é‡ä¸ä½³
- å¢åŠ é‡‡æ ·æ­¥æ•° (`num_steps`)
- ä½¿ç”¨æ›´å¥½çš„æ£€æŸ¥ç‚¹
- è°ƒæ•´æ¸©åº¦å‚æ•°

## å¼•ç”¨

å¦‚æœä½¿ç”¨æœ¬é¡¹ç›®ï¼Œè¯·è€ƒè™‘å¼•ç”¨ç›¸å…³è®ºæ–‡ï¼š

```
@article{Peebles2023DiT,
  title={Scalable Diffusion Models with Transformers},
  author={Peebles, William and Xie, Saining},
  journal={arXiv preprint arXiv:2212.09748},
  year={2023}
}

@article{Zhang2023ControlNet,
  title={Adding Conditional Control to Text-to-Image Diffusion Models},
  author={Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
  journal={arXiv preprint arXiv:2302.05543},
  year={2023}
}
```

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ Apache License 2.0 è®¸å¯è¯ã€‚