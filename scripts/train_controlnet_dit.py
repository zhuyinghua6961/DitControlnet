#!/usr/bin/env python3
"""
ControlNet-DiT Training Script for fill50k Dataset
åŸºäº DiT (Diffusion Transformer) å’Œ ControlNet çš„æ¡ä»¶å›¾åƒç”Ÿæˆè®­ç»ƒè„šæœ¬
æ”¯æŒç©ºé—´å¯¹é½å’Œå¼‚æ„æ’å€¼çš„æ•°æ®é¢„å¤„ç†
"""

import os
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import pandas as pd
from pathlib import Path
import argparse
import logging
import yaml
from tqdm import tqdm
import math
import numpy as np
from datetime import datetime
import shutil
from dataset import create_dataloaders
from models import ControlNetDiT, create_diffusion_schedule, get_loss_function
from utils import (
    train_epoch, save_checkpoint, load_config, setup_wandb,
    create_optimizer_and_scheduler, setup_device_and_precision
)

try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def verify_step_zero(model, device, resolution=512):
    """
    Step-0 è‡ªåŠ¨åŒ–æ•°å€¼éªŒè¯è„šæœ¬
    éªŒè¯é›¶åˆå§‹åŒ–ï¼ˆZero-Linearï¼‰æ˜¯å¦æˆåŠŸï¼Œç¡®ä¿æ¨¡å‹è¾“å‡ºæ­£å¸¸ä¸”ä¸åŒ…å« NaN
    """
    logger.info("ğŸ” å¼€å§‹ Step-0 æ•°å€¼éªŒè¯...")
    model.eval()
    
    # åˆ›å»ºè™šæ‹Ÿè¾“å…¥
    dummy_noise = torch.randn(1, 3, resolution, resolution).to(device)
    dummy_cond = torch.randn(1, 3, resolution, resolution).to(device)
    dummy_t = torch.zeros(1).to(device)
    
    try:
        with torch.no_grad():
            # è·å–æ¨¡å‹è¾“å‡º
            output = model(dummy_noise, dummy_cond, dummy_t)
            
        # éªŒè¯è¾“å‡º
        if torch.isnan(output).any():
            raise AssertionError("éªŒè¯å¤±è´¥ï¼šæ¨¡å‹è¾“å‡ºåŒ…å« NaN")
            
        # æ£€æŸ¥è¾“å‡ºèŒƒå›´ï¼ˆå¯¹äºæ‰©æ•£æ¨¡å‹ï¼Œè¾“å‡ºé€šå¸¸åœ¨åˆç†èŒƒå›´å†…ï¼‰
        output_mean = output.abs().mean().item()
        output_std = output.std().item()
        
        if output_mean > 10.0 or output_std > 5.0:
            logger.warning(f"âš ï¸  è¾“å‡ºæ•°å€¼è¾ƒå¤§ï¼šmean={output_mean:.4f}, std={output_std:.4f}")
        else:
            logger.info(f"âœ… è¾“å‡ºæ•°å€¼æ­£å¸¸ï¼šmean={output_mean:.4f}, std={output_std:.4f}")
            
        logger.info("âœ… Step-0 æ•°å€¼éªŒè¯é€šè¿‡ï¼šæ¨¡å‹è¾“å‡ºæ­£å¸¸ä¸” ZeroLinear å·²ç”Ÿæ•ˆ")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Step-0 éªŒè¯å¤±è´¥ï¼š{str(e)}")
        return False
    finally:
        model.train()

def main():
    parser = argparse.ArgumentParser(description='ControlNet-DiT Training')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                        help='Configuration file path')
    parser.add_argument('--resume', type=str, default=None,
                        help='Resume from checkpoint')

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    config = load_config(args.config)
    logger.info(f"Loaded configuration from {args.config}")

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    for dir_key in ['log_dir', 'checkpoint_dir', 'sample_dir']:
        Path(config['logging'][dir_key]).mkdir(exist_ok=True)

    # è®¾ç½®è®¾å¤‡å’Œæ··åˆç²¾åº¦
    device, scaler = setup_device_and_precision(config)

    # åˆå§‹åŒ–WandB
    setup_wandb(config)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_jsonl = os.path.join(config['data']['jsonl_dir'], config['data']['train_jsonl'])
    eval_jsonl = os.path.join(config['data']['jsonl_dir'], config['data']['eval_jsonl'])
    data_root = config['data']['data_dir']

    train_loader, eval_loader = create_dataloaders(
        train_jsonl=train_jsonl,
        eval_jsonl=eval_jsonl,
        data_root=data_root,
        batch_size=config['training']['batch_size'],
        resolution=config['data']['resolution'],
        num_workers=config['data']['num_workers'],
        use_bucketing=config['training']['use_bucketing']
    )

    # åˆ›å»ºæ¨¡å‹
    model = ControlNetDiT(config).to(device)

    # Step-0 æ•°å€¼éªŒè¯
    if not verify_step_zero(model, device, config['data']['resolution']):
        logger.error("Step-0 éªŒè¯å¤±è´¥ï¼Œé€€å‡ºè®­ç»ƒ")
        return

    # ç¼–è¯‘æ¨¡å‹ï¼ˆå¦‚æœæ”¯æŒï¼‰
    if config['hardware']['compile_model'] and hasattr(torch, 'compile'):
        model = torch.compile(model)
        logger.info("Model compiled with torch.compile")

    # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer, scheduler = create_optimizer_and_scheduler(model, config)

    # æ‰©æ•£è°ƒåº¦
    diffusion_schedule = create_diffusion_schedule(config)

    # æŸå¤±å‡½æ•°
    loss_fn = get_loss_function(config)

    # æ¢å¤æ£€æŸ¥ç‚¹
    start_epoch = 0
    best_loss = float('inf')
    if args.resume:
        checkpoint = torch.load(args.resume, map_location=device)
        model.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if 'scheduler_state_dict' in checkpoint:
            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        start_epoch = checkpoint['epoch'] + 1
        best_loss = checkpoint.get('loss', float('inf'))
        logger.info(f"Resumed from checkpoint: {args.resume} (epoch {start_epoch})")

    # è®­ç»ƒå¾ªç¯
    logger.info("Starting training...")
    for epoch in range(start_epoch, config['training']['num_epochs']):
        loss = train_epoch(model, train_loader, optimizer, scheduler, diffusion_schedule,
                          loss_fn, device, epoch, scaler, config)

        # è®°å½•åˆ°WandB
        if config['logging']['use_wandb'] and WANDB_AVAILABLE:
            wandb.log({
                'epoch': epoch + 1,
                'loss': loss,
                'learning_rate': optimizer.param_groups[0]['lr']
            })

        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % config['training']['save_interval'] == 0:
            save_checkpoint(model, optimizer, scheduler, epoch, loss, config)

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if loss < best_loss:
            best_loss = loss
            save_checkpoint(model, optimizer, scheduler, epoch, loss, config, is_best=True)

    logger.info("Training completed!")
    if config['logging']['use_wandb'] and WANDB_AVAILABLE:
        wandb.finish()


if __name__ == '__main__':
    main()
