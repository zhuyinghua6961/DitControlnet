# DiT-StyleTransfer 实施文档（PixArt-α + COCOStuff + WikiArt + Fill50K）

目标：在 DitControlnet 环境内，用单卡 3090 完成“内容保持 + 参考风格迁移”的训练与评测，方案严格对齐固定配置，不做额外分叉。

---

## 0. 固定选择（不要改动，作为可复现协议）

### 0.1 Backbone 与权重

- PixArt-α 官方仓库
- 预训练权重：`PixArt-XL-2-512x512`（HF）
- 训练目标与 scheduler：严格跟随该 checkpoint 的实现配置，不自行改 prediction_type

### 0.2 Teacher（伪标签）

- Teacher：**SDXL base only**
- 参考风格注入：**IP-Adapter（SDXL demo/脚本）**
- 生成分辨率：512×512
- 固定参数：
  - steps=25
  - guidance_scale=3.8
  - img2img strength=0.45
  - ip_adapter_scale=0.30
  - seeds={0,1,2}
  - sampler=DDIM（固定）
- 命名规则：`{coco_id}_{wiki_id}_seed{seed}.png`

### 0.3 数据集组合

- 内容：COCOStuff
- 风格：WikiArt（HF `huggan/wikiart`）
- 结构辅助：Fill50K
- 多任务混合比例：主任务 : Fill50K = 3 : 1

---

## 1. 环境与运行约束（DitControlnet）

- 运行环境：**DitControlnet**（所有脚本均在该环境内执行）
- 禁止在多个 Python 进程中同时加载 SDXL 或 PixArt，以免显存叠加爆掉
- 任何新脚本必须首先做小规模干跑（比如 10–50 个样本），确认显存可控再扩大

---

## 2. 目录结构约定（必须保持一致）

```
/mnt/fast18/zhu/controlnetDiT/
  data/
    coco/
    wikiart/
      raw/
      images/
      meta.csv
      splits/
        train.txt
        val.txt
        test.txt
    teacher_out/
  dataset_cocostuff/
  dataset_fill50k/
```

---

## 3. 数据准备

### 3.1 COCOStuff

- 使用 COCOStuff 的 images + captions
- captions 直接用 COCO 原始标注，不额外跑 captioner

### 3.2 WikiArt（HF 版）

- 下载 `huggan/wikiart`
- 统一整理为 `data/wikiart/images/`
- 生成 `meta.csv`，至少包含：`id,path,artist,style,genre`
- 切分比例：80/10/10
- 切分策略：按 artist 分层抽样，避免某些 artist 只出现在 test

### 3.3 Fill50K

- 使用现有 `dataset_fill50k` 目录内容
- 按原始 source/target 结构读取即可

### 3.4 镜像下载与本地整理（强制使用镜像优先）

WikiArt 使用 Hugging Face 镜像下载并整理到固定目录，避免直连不稳定：

```
HF_ENDPOINT=https://hf-mirror.com \
HF_HOME=data/wikiart/raw \
python scripts/prepare_wikiart.py \
  --output-dir data/wikiart \
  --cache-dir data/wikiart/raw \
  --mode symlink
```

生成伪对列表（teacher 生成前先准备好配对表）： 

```
python scripts/build_pseudo_pairs.py \
  --num-pairs 10000 \
  --seeds 0,1,2 \
  --output data/pairs/pseudo_pairs_train.jsonl
```

---

## 4. Teacher 伪标签生成（SDXL + IP-Adapter）

### 4.1 输入与输出

- 输入：`(I_c, I_s, prompt)`
- 输出：`I_t`（teacher target）
- 保存：`data/teacher_out/`，文件名按固定规则
- 必须记录 meta（steps/cfg/strength/seed/sampler）到 JSON 或 CSV 以便复现

### 4.2 固定生成协议（不可更改）

- steps=25
- cfg=3.5
- strength=0.40
- ip_adapter_scale=0.30
- seeds={0,1,2}
- sampler：固定 **DDIM**

> 运行排查结论：Euler 在 SDXL img2img + IP-Adapter 下更易产生坏图，teacher 生成建议固定 **DDIM**。

### 4.3 Teacher 质量控制（推荐）

- 开启自动筛选（全黑/近常数/过饱和 + 双尺度边缘结构相似度 + 高频异常比率 + CLIP 内容一致性），失败样本重试先降 `strength`，再降 `ip_adapter_scale`，同时降低 `cfg`。同时输出 A/B 质量分级与样本权重。
- 记录重试参数到 `data/teacher_out/qc_overrides.jsonl`，保持可追溯。

---

## 5. 模型改造（PixArt-α + StyleAdapter）

### 5.1 冻结策略

- 冻结：PixArt-α 主干、VAE、文本编码器、CLIP 编码器
- 训练：StyleAdapter + Purification + layer gate

### 5.2 Style Purification

- 使用内容图 CLIP embedding 作为内容代理向量
- 门控减法净化 style tokens，降低内容泄漏

### 5.3 注入层（默认 K=4）

- 选中后段 4 层注入 style
- gate 初值设为小值，避免训练初期扰动过大

---

## 6. 训练策略（单卡 3090）

- 分辨率：512
- batch size：1（必要时 2，但需先试跑）
- gradient accumulation：开启（根据显存决定累积步数）
- precision：fp16（或 bf16，按显卡支持）
- gradient checkpointing：开启
- optimizer：AdamW
- lr：1e-4（adapter+净化模块）
- warmup：500 steps
- checkpoint 保存：每 2k steps

---

## 7. 评测协议（最小可行）

- 风格像：WikiArt 风格分类 Top-1
- 内容保持：LPIPS / DINO
- 文本一致：CLIPScore
- 效率：额外参数量 + 峰值显存 + 推理耗时

---

## 8. 消融设计（保持轻量）

- K=2 / 4 / 8 / All
- 净化模块：None / 投影 / 门控减法
- 步数调度：无 / 分段 / 余弦
- 多任务：无 / 加 Fill50K

---

## 9. 显存与内存管理（必须遵守，避免死机）

### 9.1 GPU 显存（避免 OOM）

- 所有训练与生成必须使用 `fp16`（或 `bf16`），禁止 `fp32`
- 开启 `gradient checkpointing`
- 生成伪标签时使用 `torch.no_grad()`
- 生成伪标签时 batch=1，逐张写盘后释放张量
- SDXL 推理务必启用 memory efficient attention（如可用）
- 不要在同一进程里同时加载 SDXL 与 PixArt
- 默认关闭 EMA（会额外占用显存）；若必须用，先确认显存余量再开启

### 9.2 CPU 内存（避免 RAM 爆掉）

- 数据读取必须按需加载，禁止将整个数据集加载到内存
- 不做全量 list/metadata 的一次性读取到内存，索引与样本路径按需生成/迭代
- dataloader 配置严格限制：`batch_size=1`、`num_workers=2`（必要时降到 0/1）、`prefetch_factor=1`、`persistent_workers=False`
- 禁用/谨慎使用 `pin_memory`（会增加 RAM 占用），默认 `pin_memory=False`
- 关闭不必要的缓存与中间结果保存，避免 RAM 持续上涨
- 评测/日志只做流式累计（写盘或增量统计），禁止把所有生成图/中间特征常驻内存
- 生成伪标签时按单样本流式写盘，禁止把生成结果堆在内存里再集中保存
- 每次阶段性任务结束后，明确退出进程释放 RAM（避免内存泄漏累积）

### 9.3 运行规范（强制）

- 每次长流程分段执行：
  - 先生成少量 teacher
  - 再训练小步数
  - 再扩大规模
- 每阶段结束手动释放显存（退出进程，重启再跑下一阶段）
- 不要同时开多个训练/生成脚本
- 若出现显存碎片化，优先重启进程，而不是继续堆积
- 若出现 RAM 持续上涨或系统响应变慢，立刻停止当前进程并分段运行
- RAM/显存使用率超过 85% 时必须暂停扩规模，先降并行度和预取，再继续

---

## 10. 执行顺序（最小可行闭环）

1. 准备 WikiArt 目录与 meta.csv
2. 生成 10k teacher targets（seed=0）验证闭环
3. 训练 StyleAdapter（只跑 20k steps）
4. 评测风格/内容/文本指标，确认趋势
5. 再扩展到 50k/200k 规模

---

## 11. 论文表述关键句（可直接用）

- We follow the original PixArt-α training objective and scheduler configuration associated with the released checkpoint, and only optimize parameters in the proposed StyleAdapter modules.
- Teacher targets are generated by SDXL base with IP-Adapter under a fixed protocol (512, 25 steps, CFG=3.8, strength=0.45, ip_scale=0.30, DDIM, seeds {0,1,2}).
- To prevent OOM on a single 3090, we freeze the backbone, enable mixed precision and gradient checkpointing, and keep batch size at 1 with gradient accumulation.
