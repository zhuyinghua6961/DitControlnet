# --- 核心深度学习框架 (建议匹配 CUDA 12.1) ---
torch>=2.1.0
torchvision
torchaudio

# --- Hugging Face 生态系统 ---
# 建议通过 git+https://github.com/huggingface/diffusers.git 安装最新开发版以支持 DiT 控制器
diffusers>=0.26.0
transformers>=4.38.0
accelerate>=0.27.0
peft>=0.8.0
datasets
safetensors

# --- 显存与计算优化 (RTX 3090Ti 核心) ---
# bitsandbytes 提供 8-bit AdamW 优化器 [cite: 62, 71]
bitsandbytes>=0.42.0
# xformers 提供内存高效的 Attention 实现 [cite: 61, 69]
xformers>=0.0.23
# flash-attn 显著降低显存并提升 DiT 计算速度 [cite: 448-451]
# 注意: flash-attn 安装较慢,可先安装其他依赖,最后单独安装
# 安装命令: pip install flash-attn --no-build-isolation (需要较长时间)
# flash-attn>=2.5.0

# --- 数据处理与图像工程 ---
opencv-python
imageio
matplotlib
scipy
tqdm
PyYAML

# --- 核心深度学习框架 (建议匹配 CUDA 12.1) ---
torch>=2.1.0
torchvision
torchaudio

# --- Hugging Face 生态系统 ---
# 建议通过 git+https://github.com/huggingface/diffusers.git 安装最新开发版以支持 DiT 控制器
diffusers>=0.26.0
transformers>=4.38.0
accelerate>=0.27.0
peft>=0.8.0
datasets
safetensors

# --- 显存与计算优化 (RTX 3090Ti 核心) ---
# bitsandbytes 提供 8-bit AdamW 优化器 [cite: 62, 71]
bitsandbytes>=0.42.0
# xformers 提供内存高效的 Attention 实现 [cite: 61, 69]
xformers>=0.0.23
# flash-attn 显著降低显存并提升 DiT 计算速度 [cite: 448-451]
# 注意: flash-attn 安装较慢,可先安装其他依赖,最后单独安装
# 安装命令: pip install flash-attn --no-build-isolation (需要较长时间)
# flash-attn>=2.5.0

# --- 数据处理与图像工程 ---
opencv-python
imageio
matplotlib
scipy
tqdm
PyYAML

# --- 评估与监控 ---
clean-fid>=0.1.35
wandb

# --- 评估指标依赖 ---
scikit-image  # 用于 SSIM 计算
opencv-python  # 用于边缘检测
ftfy  # CLIP 依赖
regex  # CLIP 依赖

# --- Baseline 实验特定依赖 ---
# 用于 PixArt-alpha-XL-2 和 ControlNet
sentencepiece  # 用于 T5 tokenizer